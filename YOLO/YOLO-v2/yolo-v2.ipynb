{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import scipy\n",
    "import cv2\n",
    "import gc\n",
    "\n",
    "#解析使用\n",
    "import xml\n",
    "from xml.etree import ElementTree as ET\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.applications import VGG19\n",
    "from keras.models import Model\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import imageio\n",
    "from skimage import transform\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.svm import SVC #类别分类使用\n",
    "from sklearn.linear_model import Ridge #bounding-box回归\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.contrib import slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action='error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN_DATA_PATH = '../../../tensorflow2/dataset/VOCtrainval_11-May-2012/JPEGImages/'\n",
    "TEST_DATA_PATH = '../../../tensorflow2/dataset/VOC2012test/JPEGImages/'\n",
    "\n",
    "TRAIN_XML_PATH = '../../../tensorflow2/dataset/VOCtrainval_11-May-2012/Annotations/'\n",
    "TEST_XML_PATH = '../../../tensorflow2/dataset/VOC2012test/Annotations/'\n",
    "\n",
    "CLASSES_NUM = 20\n",
    "\n",
    "STR = [\n",
    "    'person',\n",
    "    'bird','cat','cow','dog','horse','sheep',\n",
    "    'aeroplane','bicycle','boat','bus','car','motorbike','train',\n",
    "    'bottle','chair','diningtable','pottedplant','sofa','tvmonitor'\n",
    "]\n",
    "\n",
    "LABEL2STR = {idx:value for idx , value in enumerate(STR)}\n",
    "STR2LABEL = {value:key for key,value in LABEL2STR.items()}\n",
    "#STR2LABEL = {value:idx for idx , value in enumerate(STR)}\n",
    "\n",
    "STR2LABEL['none'] = 'none' #先不使用part部分 只进行naive目标检测\n",
    "\n",
    "#目标检测相关\n",
    "IoU_THRESHOLD = 0.5\n",
    "\n",
    "#SVM相关\n",
    "SVM_IoU_THRESHOLD = 0.3\n",
    "\n",
    "#NMS相关\n",
    "NMS_IoU_THRESHOLD = 0.3 #or ~0.5\n",
    "\n",
    "#bbox回归\n",
    "BBOX_REGRESS_IoU_THRESHOLD = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xml_file_names_train = glob(TRAIN_XML_PATH + '*') #所有的xml文件 完整路径\n",
    "\n",
    "#从xml文件中读出图片相关的信息\n",
    "\n",
    "def xml_parse(xml_file):\n",
    "    '''\n",
    "    return filename , shape , name_boxes , crop_boxes\n",
    "    xml文件中的shape格式为 (width height 3)\n",
    "    '''\n",
    "    xml_file = xml.dom.minidom.parse(xml_file)\n",
    "    xml_file_docu_ele = xml_file.documentElement\n",
    "\n",
    "    filename_list = xml_file_docu_ele.getElementsByTagName('filename')\n",
    "    \n",
    "    #filename_list可能有多个filename的 所以要索引0(此数据集中filename只有一个)\n",
    "    filename = filename_list[0].childNodes[0].data #filename_list.firstChild.data\n",
    "\n",
    "    #图像的尺寸信息\n",
    "    size_list = xml_file_docu_ele.getElementsByTagName('size')\n",
    "\n",
    "    for size in size_list:\n",
    "        width_list = size.getElementsByTagName('width')\n",
    "        width = int(width_list[0].childNodes[0].data)\n",
    "\n",
    "        height_list = size.getElementsByTagName('height')\n",
    "        height = int(height_list[0].childNodes[0].data)\n",
    "\n",
    "        channel_list = size.getElementsByTagName('depth')\n",
    "        channel = int(channel_list[0].childNodes[0].data)\n",
    "\n",
    "    #一个文件中有多个object\n",
    "    object_list = xml_file_docu_ele.getElementsByTagName('object')\n",
    "\n",
    "    #多个object与多个object对应的详细信息\n",
    "    name_boxes = [] #一个元素就是一个object\n",
    "    crop_boxes = []\n",
    "\n",
    "    for objects in object_list:\n",
    "        #一次循环处理一个object信息\n",
    "        #一个xml文件（即一个图像中）有多个object\n",
    "\n",
    "        #name\n",
    "        name_list = objects.getElementsByTagName('name')\n",
    "\n",
    "        name_box = name_list[0].childNodes[0].data\n",
    "\n",
    "        #bounding box points\n",
    "        bndbox = objects.getElementsByTagName('bndbox')\n",
    "\n",
    "        x1_list = bndbox[0].getElementsByTagName('xmin')\n",
    "        x1 = int( round( float(x1_list[0].childNodes[0].data) ) )\n",
    "\n",
    "        y1_list = bndbox[0].getElementsByTagName('ymin')\n",
    "        y1 = int(round(float( y1_list[0].childNodes[0].data )))\n",
    "\n",
    "        x2_list = bndbox[0].getElementsByTagName('xmax')\n",
    "        x2 = int(round(float( x2_list[0].childNodes[0].data )))\n",
    "\n",
    "        y2_list = bndbox[0].getElementsByTagName('ymax')\n",
    "        y2 = int(round(float( y2_list[0].childNodes[0].data )))\n",
    "\n",
    "        crop_box = [x1,x2,y1,y2]\n",
    "\n",
    "        name_boxes.append(name_box)\n",
    "        crop_boxes.append(crop_box)\n",
    "\n",
    "    #crop_box:[x1 x2 y1 y2]\n",
    "    return filename , name_boxes , np.array(crop_boxes) #filename调试使用\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#xml_parse(xml_file_names_train[897])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Image(object):\n",
    "    '''\n",
    "    图片的真实信息\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.img_file_names_train = glob(TRAIN_DATA_PATH+'*') #训练全路径信息\n",
    "                \n",
    "    def load(self , img_path_name = None):\n",
    "        if not img_path_name:\n",
    "            img_path_name = np.random.choice(self.img_file_names_train) #随机选择一张图片\n",
    "            #img_path_idx = np.random.randint(0 , high = len(self.img_file_names_train)) #随机索引\n",
    "\n",
    "        img_arr = cv2.imread(img_path_name) #BGR height*width*chanel\n",
    "        \n",
    "        xml_file_name = TRAIN_XML_PATH + img_path_name[-15:-4] +  '.xml'\n",
    "        \n",
    "        _ , name_boxes , crop_boxes = xml_parse(xml_file_name)\n",
    "        \n",
    "        labels = [] #存储与bndbox对应的 label信息\n",
    "\n",
    "        for i in range(len(crop_boxes)): #多个object \n",
    "            labels.append(STR2LABEL.get(name_boxes[i] , 'none'))\n",
    "        \n",
    "        return img_arr , labels , crop_boxes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reference: github:sualab\n",
    "class Img_generator(object):\n",
    "    def __init__(self):\n",
    "        self.img_loader = Image()\n",
    "\n",
    "    #计算bbox面积\n",
    "    def bbox_area(self , bbox):\n",
    "        w = bbox[1] - bbox[0]\n",
    "        h = bbox[3] - bbox[2]\n",
    "        \n",
    "        return w*h\n",
    "    \n",
    "    #计算交并比\n",
    "    def IoU(self , bbox_a , bbox_b):\n",
    "        xmin_a = bbox_a[0]\n",
    "        xmax_a = bbox_a[1]\n",
    "        ymin_a = bbox_a[2]\n",
    "        ymax_a = bbox_a[3]\n",
    "        \n",
    "        xmin_b = bbox_b[0]\n",
    "        xmax_b = bbox_b[1]\n",
    "        ymin_b = bbox_b[2]\n",
    "        ymax_b = bbox_b[3]\n",
    "        \n",
    "        if   xmin_a < xmax_b <= xmax_a and (ymin_a < ymax_b <= ymax_a or ymin_a <= ymin_b < ymax_a):\n",
    "            flag = True\n",
    "        elif xmin_a <= xmin_b < xmax_a and (ymin_a < ymax_b <= ymax_a or ymin_a <= ymin_b < ymax_a):\n",
    "            flag = True\n",
    "        elif xmin_b < xmax_a <= xmax_b and (ymin_b < ymax_a <= ymax_b or ymin_b <= ymin_a < ymax_b):\n",
    "            flag = True\n",
    "        elif xmin_b <= xmin_a < xmax_b and (ymin_b < ymax_a <= ymax_b or ymin_b <= ymin_a < ymax_b):\n",
    "            flag = True\n",
    "        else:\n",
    "            flag = False\n",
    "        \n",
    "        if flag:\n",
    "            x_sorted_list = sorted([xmin_a, xmax_a, xmin_b, xmax_b])\n",
    "            y_sorted_list = sorted([ymin_a, ymax_a, ymin_b, ymax_b])\n",
    "            \n",
    "            x_intersect_w = x_sorted_list[2] - x_sorted_list[1] #0 1 2 3\n",
    "            y_intersect_h = y_sorted_list[2] - y_sorted_list[1] #0 1 2 3\n",
    "            \n",
    "            area_inter = x_intersect_w * y_intersect_h #计算重合面积\n",
    "            \n",
    "            union_area = self.bbox_area(bbox_a) + self.bbox_area(bbox_b) - area_inter\n",
    "            \n",
    "            return area_inter/union_area\n",
    "        else:\n",
    "            return 0.0\n",
    "    \n",
    "        \n",
    "    def get_train_proposal(self , ground_truth_labels , ground_truth_coord , img_shape): #img_shape:[height width 3] \n",
    "        #[[1.3221,1.73145],[3.19275,4.00944],[5.05587,8009892],[9.47112,4.84053],[11.2364,10.0071]]\n",
    "        #将上述的anchor信息（13*13坐标系中） 乘以32 转换为416*416坐标系中\n",
    "        anchors = [[42.3072,55.4064],[102.168,128.30208],[161.78784,256.316544],[303.07584,154.89696],[359.5648,320.2272]] #只有宽高信息\n",
    "        #grid cell尺寸 feature map尺寸\n",
    "        grid_w = 13\n",
    "        grid_h = 13\n",
    "        \n",
    "        oh = img_shape[0] #原图height\n",
    "        ow = img_shape[1] #原图width\n",
    "        \n",
    "        labels = []\n",
    "        label = np.zeros((13 , 13 , 5 , 5+20))\n",
    "        \n",
    "        for idx , (x_min , x_max , y_min , y_max) in enumerate(ground_truth_coord):\n",
    "            x_min , y_min , x_max , y_max = x_min/ow , y_min/oh , x_max/ow , y_max/oh #变为在原图中的比例\n",
    "            x_min , y_min , x_max , y_max = np.clip([x_min , y_min , x_max , y_max] , a_min=0.0 , a_max=1.0)\n",
    "        \n",
    "            anchor_boxes = np.array(anchors) / np.array([ow , oh]) #将anchors转换为在原图中的比例\n",
    "            \n",
    "            best_anchor = self._get_best_anchor(anchor_boxes , [x_max-x_min , y_max-y_min])\n",
    "            \n",
    "            #当前ground truth的中点落在哪一个grid cell中\n",
    "            cx = int(np.floor((x_min+x_max)/2) * grid_w)\n",
    "            cy = int(np.floor((y_min+y_max)/2) * grid_h)\n",
    "            \n",
    "            label[cy , cx , best_anchor , 0:4] = [x_min , y_min , x_max , y_max]\n",
    "            label[cy , cx , best_anchor , 4] = 1.0\n",
    "            label[cy , cx , best_anchor , 5+ground_truth_labels[idx]] = 1.0\n",
    "        \n",
    "        labels.append(label)\n",
    "        \n",
    "        return np.array(labels)\n",
    "    \n",
    "    def _get_best_anchor(self , anchors , box_wh):\n",
    "        '''\n",
    "        此处使用的坐标均在(0 1)范围内\n",
    "        \n",
    "        此处计算iou的时候 不考虑坐标位置 只考虑height and width\n",
    "        '''\n",
    "        box_wh = np.array(box_wh)\n",
    "        \n",
    "        best_iou = 0.0\n",
    "        best_anchor = 0 #最好anchor的索引\n",
    "        \n",
    "        for k , anchor in enumerate(anchors):\n",
    "            intersect_wh = np.maximum(np.minimum(box_wh , anchor) , 0.0)\n",
    "            intersect_area = intersect_wh[0] * intersect_wh[1]\n",
    "            \n",
    "            box_area = box_wh[0] * box_wh[1]\n",
    "            anchor_area = anchor[0] * anchor[1]\n",
    "            \n",
    "            iou = intersect_area / (box_area+anchor_area-intersect_area)\n",
    "            \n",
    "            if iou > best_iou:\n",
    "                best_iou = iou\n",
    "                best_anchor = k\n",
    "            \n",
    "        return best_anchor\n",
    "    \n",
    "    def load(self , img_path_name):\n",
    "        '''\n",
    "        img_path_name:绝对路径\n",
    "        '''\n",
    "        \n",
    "        #图片数据 label ground_truth坐标信息\n",
    "        img_arr , ground_truth_labels , ground_truth_coord = self.img_loader.load(img_path_name)\n",
    "        \n",
    "        labels = self.get_train_proposal(ground_truth_labels , ground_truth_coord , img_arr.shape)\n",
    "        \n",
    "        img_arr = cv2.resize(img_arr , (416 , 416))\n",
    "        img_arr = img_arr / 127.5 - 1 #对下面的get_train_proposal没有影响\n",
    "        \n",
    "        '''\n",
    "        resize 并 归一化像素值\n",
    "        img_arr 为 BGR形式\n",
    "        '''\n",
    "        \n",
    "        #[R G B] [123.68 116.779 103.939]\n",
    "        #减去每个通道的像素平均值 归一化\n",
    "        #因为cv2打开的形式为BGR\n",
    "        #img_arr[:,:,0] = img_arr[:,:,0] - 103.939\n",
    "        #img_arr[:,:,1] = img_arr[:,:,1] - 116.779\n",
    "        #img_arr[:,:,2] = img_arr[:,:,2] - 123.680\n",
    "        \n",
    "        return np.expand_dims(img_arr , axis=0) , labels\n",
    "    \n",
    "    \n",
    "    def load_test(self , img_path_name):\n",
    "        img_arr = cv2.imread(img_path_name)\n",
    "        \n",
    "        img_arr_resize = cv2.resize(img_arr , (448 , 448))\n",
    "        img_arr_resize_norm = img_arr_resize / 127.5 - 1\n",
    "        \n",
    "        return np.expand_dims(img_arr_resize_norm , axis=0) , img_arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    def __init__(self):\n",
    "        self.img_generator = Img_generator()\n",
    "        \n",
    "        self.img_loader = Image()\n",
    "        \n",
    "        self.img_file_names_train = glob(TRAIN_DATA_PATH + '*')\n",
    "        self.img_file_names_test = glob(TEST_DATA_PATH + '*')\n",
    "    \n",
    "    def get_batch(self):\n",
    "        path = np.random.choice(self.img_file_names_train)\n",
    "        \n",
    "        x , labels = self.img_generator.load(path)\n",
    "    \n",
    "        return x , labels\n",
    "    \n",
    "    def get_batch_test(self , path):\n",
    "        \n",
    "        if not path:\n",
    "            #未指定path 从测试目录中随机选一张图片测试\n",
    "            path = np.random.choice(self.img_file_names_test)\n",
    "            print('test image:', path)\n",
    "        \n",
    "        \n",
    "        x , img_arr= self.img_generator.load_test(path)\n",
    "        \n",
    "        return x , img_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = Dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b = test.get_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DarkNet(object):\n",
    "    def __init__(self , is_training=True):\n",
    "        \n",
    "        self.x = tf.placeholder(dtype=tf.float32 , shape=[1 , 416 , 416 , 3])        \n",
    "        \n",
    "        self.build() #构建网络产生输出\n",
    "        \n",
    "        if is_training:\n",
    "            self.labels = tf.placeholder(dtype=tf.float32 , shape=[1 , 13 , 13 , 5 , (5+20)])\n",
    "            self.loss()\n",
    "\n",
    "    def build(self):\n",
    "        #arch from paper\n",
    "        def _batch_norm(_input):\n",
    "            return slim.batch_norm(_input)\n",
    "        \n",
    "        def _weight_variable(shape , name):\n",
    "            return tf.get_variable('weights_'+name , shape=shape , dtype=tf.float32 ,\n",
    "                                    initializer = tf.initializers.truncated_normal(stddev=0.01) , trainable = True)\n",
    "        \n",
    "        def _bias_variable(shape , name):\n",
    "            return tf.get_variable('biases_'+name , shape=shape , dtype=tf.float32 ,\n",
    "                                    initializer = tf.initializers.constant(0.0))\n",
    "        \n",
    "        def _conv(_input , num_outputs , kernel_size , stride=1 , padding='SAME' , name='default' , is_activation=True):\n",
    "            weight = _weight_variable(shape=[kernel_size , kernel_size , _input.get_shape().as_list()[-1] , num_outputs] , name=name)\n",
    "            biases = _bias_variable(shape=[num_outputs] , name=name)\n",
    "            \n",
    "            if is_activation:\n",
    "                #conv->norm->relu [->pooling]\n",
    "                return tf.nn.leaky_relu( _batch_norm( tf.nn.conv2d(_input , weight , strides=[1,stride,stride,1] , padding=padding) + biases ) ,\n",
    "                                     alpha=0.1)\n",
    "            else:\n",
    "                #conv\n",
    "                return tf.nn.conv2d(_input , weight , strides=[1,stride,stride,1] , padding=padding) + biases\n",
    "                          \n",
    "        def _max_pool(_input , kernel_size=2 , stride=2 , padding='VALID'):\n",
    "            return slim.max_pool2d(_input , kernel_size=kernel_size , stride=stride)\n",
    "        \n",
    "        \n",
    "        #_conv中已放入batch-norm\n",
    "        #darknet-19\n",
    "        output = _conv(self.x , 32 , 3 , name='conv1')\n",
    "        output = _max_pool(output)\n",
    "           \n",
    "        output = _conv(output , 64 , 3 , name='conv2')              \n",
    "        output = _max_pool(output)\n",
    "        \n",
    "        output = _conv(output , 128 , 3 , name='conv3')\n",
    "        output = _conv(output , 64 , 1 , name='conv4')\n",
    "        output = _conv(output , 128 , 3 , name='conv5')\n",
    "        output = _max_pool(output)\n",
    "                \n",
    "        output = _conv(output , 256 , 3 , name='conv6')\n",
    "        output = _conv(output , 128 , 1 , name='conv7')\n",
    "        output = _conv(output , 256 , 3 , name='conv8')\n",
    "        output = _max_pool(output)\n",
    "        \n",
    "        output = _conv(output , 512 , 3 , name='conv9')\n",
    "        output = _conv(output , 256 , 1 , name='conv10')\n",
    "        output = _conv(output , 512 , 3 , name='conv11')\n",
    "        output = _conv(output , 256 , 1 , name='conv12')\n",
    "        output = _conv(output , 512 , 3 , name='conv13')\n",
    "        #(26 26 512)\n",
    "        fine_grained = output #细粒度\n",
    "        \n",
    "        output = _max_pool(output) #细粒度 passthrough layer需要\n",
    "        #此时shape为（13 13 512）\n",
    "        \n",
    "        output = _conv(output , 1024 , 3 , name='conv14')\n",
    "        output = _conv(output , 512 , 1 , name='conv15')\n",
    "        output = _conv(output , 1024 , 3 , name='conv16')\n",
    "        output = _conv(output , 512 , 1 , name='conv17')\n",
    "        output = _conv(output , 1024 , 3 , name='conv18')\n",
    "        \n",
    "        #detection arch\n",
    "        output = _conv(output , 1024 , 3 , name='conv19')\n",
    "        output = _conv(output , 1024 , 3 , name='conv20')\n",
    "        #(13 13 1024)\n",
    "        '''\n",
    "        细粒度与粗粒度合并\n",
    "        '''\n",
    "        fine_grained = _conv(fine_grained , 64 , 1 , name='passthrough')\n",
    "        #(26 26 64)\n",
    "        fine_grained = tf.space_to_depth(fine_grained , block_size=2)\n",
    "        #(13 13 256)\n",
    "        \n",
    "        output = tf.concat((fine_grained , output) , axis=-1) #(13 13 256+1024) == (13 13 1280)\n",
    "\n",
    "        output = _conv(output , 1024 , 3 , name='conv21')\n",
    "        \n",
    "        #最后一层不归一 不激活 不池化\n",
    "        output = _conv(output , (5*(20+5)) , 1 , name='conv22' , is_activation=False)\n",
    "        \n",
    "        self.output = tf.reshape(output , shape=(-1 , 13 , 13 , 5 , 25))\n",
    "        \n",
    "        \n",
    "    def loss(self):\n",
    "        '''\n",
    "        reference: github:sualab\n",
    "        '''\n",
    "                \n",
    "        #self.labels\n",
    "        \n",
    "        loss_weights = [5.0 , 5.0 , 5.0 , 0.5 , 1.0]\n",
    "        grid_h = 13\n",
    "        grid_w = 13\n",
    "        num_classws = 20\n",
    "        anchors = np.array( [[42.3072,55.4064],[102.168,128.30208],[161.78784,256.316544],[303.07584,154.89696],[359.5648,320.2272]] ) #可以不需要进行np.array转换 直接可以用np函数处理\n",
    "        \n",
    "        cxcy = np.transpose([ np.tile(np.arange(grid_w) , grid_h) , np.repeat(np.arange(grid_h) , grid_w) ])\n",
    "        cxcy = np.reshape(cxcy , (1,grid_h , grid_w , 1 , 2))\n",
    "        \n",
    "        '''\n",
    "        将网络的输出进行切分\n",
    "        '''\n",
    "        txty = self.output[:,:,:,:,0:2]\n",
    "        twth = self.output[:,:,:,:,2:4]\n",
    "        confidence = tf.sigmoid(self.output[:,:,:,:,4])\n",
    "        class_probs = tf.nn.softmax(self.output[:,:,:,:,5:] , axis=-1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        lambda_coord = 5.0\n",
    "        lambda_noobj = 0.5\n",
    "        \n",
    "        #[:,:,0]\n",
    "        _mask = tf.cast( tf.greater( tf.slice(self.target,begin=[0,0,0],size=[7,7,1]) , np.zeros(shape=[7,7,1] , dtype=float) ) , dtype=tf.float32 )\n",
    "        mask = tf.tile(_mask , multiples=[1 , 1 , 2]) #7*7*2\n",
    "        #[:,:,[1,2]]\n",
    "        self.loss_coord = tf.reduce_sum( tf.square( ( tf.slice(self.output,begin=[0,0,1],size=[7,7,2]) - tf.slice(self.target,begin=[0,0,1],size=[7,7,2]) ) * mask ) )\n",
    "        \n",
    "        '''========'''\n",
    "        '''此处不修正 不出现给负数开方 出现nan情况'''\n",
    "        #[:,:,[3,4]]\n",
    "        self.loss_coord += tf.reduce_sum( tf.square( (tf.sqrt( tf.slice(self.output,begin=[0,0,3],size=[7,7,2]) ) - tf.sqrt( tf.slice(self.target,begin=[0,0,3],size=[7,7,2]))) * mask ) )\n",
    "        '''========'''\n",
    "        \n",
    "        self.loss_coord = lambda_coord * self.loss_coord\n",
    "        \n",
    "        #[:,:,:[0,5]]\n",
    "        self.loss_iou = tf.reduce_sum( tf.square( ( tf.slice(self.output,begin=[0,0,0],size=[7,7,1]) - tf.slice(self.target,begin=[0,0,0],size=[7,7,1])) * mask ) )\n",
    "        self.loss_iou += tf.reduce_sum( tf.square( ( tf.slice(self.output,begin=[0,0,5],size=[7,7,1]) - tf.slice(self.target,begin=[0,0,5],size=[7,7,1])) * mask ) )\n",
    "        #[:,:,:[0,5]]\n",
    "        self.loss_iou += lambda_noobj * (tf.reduce_sum( tf.square( ( tf.slice(self.output,begin=[0,0,0],size=[7,7,1]) - tf.slice(self.target,begin=[0,0,0],size=[7,7,1]) ) * (1.0-mask) ) ) +\\\n",
    "                                    tf.reduce_sum( tf.square( ( tf.slice(self.output,begin=[0,0,5],size=[7,7,1]) - tf.slice(self.target,begin=[0,0,5],size=[7,7,1]) ) * (1.0-mask) ) ))\n",
    "        \n",
    "        mask = tf.tile(_mask , multiples=[1,1,20]) #7*7*20 (因为20 classes)\n",
    "        #[:,:,10:]\n",
    "        self.loss_cls = tf.reduce_sum( tf.square( ( tf.slice(self.output,begin=[0,0,10],size=[7,7,20]) - tf.slice(self.target,begin=[0,0,10],size=[7,7,20]) ) * mask ) )\n",
    "        \n",
    "        self.total_loss = self.loss_coord + self.loss_iou + self.loss_cls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test = DarkNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def display(img_arr , labels , bbox , name):    \n",
    "    \n",
    "    print('debug:' , img_arr.shape , labels , bbox)\n",
    "    \n",
    "    for i in range(len(labels)):\n",
    "\n",
    "        x1 = bbox[i][0]\n",
    "        x2 = bbox[i][1]\n",
    "        y1 = bbox[i][2]\n",
    "        y2 = bbox[i][3]\n",
    "\n",
    "        img_arr = cv2.rectangle(img_arr , (x1 , y1) , (x2 , y2) , (255,255,255))\n",
    "\n",
    "        img_arr = cv2.putText(img_arr , LABEL2STR[labels[i]] , org=(x1 , y1+10) , fontFace = cv2.FONT_HERSHEY_PLAIN , fontScale=1 , color = (255,255,255), thickness = 1)\n",
    "\n",
    "    #plt.imshow(meta_img) #图像查看\n",
    "\n",
    "    plt.imsave(arr=img_arr[: , : ,[2,1,0]] , fname = 'result/%s.jpg' % name) #保存图像\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#refer:https://blog.csdn.net/two_vv/article/details/76769860\n",
    "\n",
    "class YOLO_V2(object):\n",
    "    '''\n",
    "    完整模型\n",
    "    '''\n",
    "    \n",
    "    def __init__(self , is_training = True):\n",
    "        self._grid() #创建grid cell 坐标信息\n",
    "        \n",
    "        self.dataset = Dataset()\n",
    "        \n",
    "        self.filewriter_path = 'save/logs' #模型可视化\n",
    "        self.checkpoint_path = 'save/model/' #模型持久化\n",
    "                              \n",
    "        self.model = DarkNet(is_training)\n",
    "        \n",
    "        self.sess = tf.Session()\n",
    "        \n",
    "        self.saver = tf.train.Saver(max_to_keep=2) #max_to_keep 最大保存5次模型  之后继续保存则会覆盖前面的模型\n",
    "        \n",
    "        if is_training:\n",
    "            '''训练参数'''\n",
    "            self.epoch = 100000\n",
    "            \n",
    "            self.global_step = tf.Variable(initial_value=0 , trainable=False)\n",
    "            \n",
    "            self.learning_rate = tf.train.exponential_decay(learning_rate=0.00001 , global_step=self.global_step,\n",
    "                                                            decay_steps=900 , decay_rate=0.8 , staircase=True)\n",
    "            \n",
    "            self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.model.total_loss , global_step=self.global_step)\n",
    "        \n",
    "            #引入滑动平均\n",
    "            self.ema = tf.train.ExponentialMovingAverage(decay=0.9) #滑动平均\n",
    "            self.average_op = self.ema.apply(tf.trainable_variables()) #给所有的可训练变量应用滑动平均\n",
    "            \n",
    "            with tf.control_dependencies([self.optimizer]):\n",
    "                self.train_op = tf.group(self.average_op)\n",
    "            \n",
    "            '''可视化'''\n",
    "            self.sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "            tf.summary.scalar('total_loss' , self.model.total_loss)\n",
    "            self.merged_summary = tf.summary.merge_all() #merge all summaries in the default graph\n",
    "            self.writer = tf.summary.FileWriter(self.filewriter_path , self.sess.graph) #可视化\n",
    "            \n",
    "    \n",
    "    def _grid(self):\n",
    "        #S*S grid cells\n",
    "        x_slice = [ [64*i , 64*(i+1)] for i in [0,1,2,3,4,5,6] ]\n",
    "        y_slice = [ [64*i , 64*(i+1)] for i in [0,1,2,3,4,5,6] ]\n",
    "        \n",
    "        self.grid = np.zeros(shape=[7 , 7] , dtype=list)\n",
    "        \n",
    "        for x_idx , x in enumerate(x_slice):\n",
    "            for y_idx , y in enumerate(y_slice):\n",
    "                self.grid[x_idx][y_idx] = y + x\n",
    "            \n",
    "    def train(self):\n",
    "        \n",
    "        if os.path.exists(self.checkpoint_path+'checkpoint'):\n",
    "            self.saver.restore(self.sess , tf.train.latest_checkpoint(self.checkpoint_path))\n",
    "        else:\n",
    "            self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for i in range(300):\n",
    "            '''\n",
    "            构造target的时候（即构造训练集的时候）每一个grid cell中只构造一个bounding box 即7*7*(1+4+20) #confidence_score(iou)+offset+p_i\n",
    "            '''\n",
    "            x , labels = self.dataset.get_batch()\n",
    "            \n",
    "            self.sess.run(self.train_op , feed_dict={self.model.x : x , self.labels.target : labels} )\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                self.saver.save(self.sess , self.checkpoint_path + 'model.ckpt' , global_step = i)\n",
    "                \n",
    "                total_loss , summary = self.sess.run([self.model.total_loss , self.merged_summary] , feed_dict={self.model.x : x , self.model.labels : labels})\n",
    "                        \n",
    "                self.writer.add_summary(summary , global_step = i)\n",
    "                                \n",
    "                print(i , total_loss)\n",
    "            \n",
    "        self.writer.close() #event to disk and close the file\n",
    "\n",
    "    def predict(self , path=None):\n",
    "        if os.path.exists(self.checkpoint_path + 'checkpoint'):\n",
    "            self.saver.restore(self.sess , tf.train.latest_checkpoint(self.checkpoint_path) )\n",
    "            \n",
    "            self._predict(path)\n",
    "        else:\n",
    "            print('no model!!!')\n",
    "            return \n",
    "            \n",
    "    def _predict(self , path):\n",
    "        threshold_1 = 0.01\n",
    "        threshold_nms = 0.7\n",
    "        \n",
    "        x , img_arr= self.dataset.get_batch_test(path)\n",
    "        \n",
    "        output = self.sess.run(self.model.output , feed_dict={self.model.x : x})\n",
    "        self._output = output\n",
    "        \n",
    "        prod = self._confidence_p(output)\n",
    "        bbox = self._bbox(output)\n",
    "                \n",
    "        pred_labels = []\n",
    "        pred_bnds = []\n",
    "        \n",
    "        #step 1 set zero\n",
    "        prod = np.greater(prod , threshold_1).astype(dtype=int) * prod #小于阈值置0\n",
    "        \n",
    "        #step 2 sort and nms\n",
    "        for i in range(20):\n",
    "            des_idx = np.argsort(-1 * prod[i]) #降序排列\n",
    "            \n",
    "            nms_idx = self._nms(prod[i][des_idx] , bbox[: , des_idx] , des_idx , threshold_nms) #返回的对应被丢弃的bnd box索引\n",
    "            \n",
    "            #将丢弃的confidence*pi置0\n",
    "            prod[i][nms_idx] = 0\n",
    "        \n",
    "        #step 3 final prediction\n",
    "        for i in range(98): #一次处理每一个bounding box\n",
    "            if np.max(prod[: , i]) != 0:\n",
    "                pred_idx = np.argmax(prod[: , i])\n",
    "                \n",
    "                pred_labels.append(pred_idx)\n",
    "                pred_bnds.append( self._to_original(bbox[: , i] , img_arr.shape) )\n",
    "        \n",
    "        #step 4 draw in image\n",
    "        display(img_arr , pred_labels , pred_bnds , 'first')\n",
    "        \n",
    "    def _to_original(self , bbox , shape):\n",
    "        return [int( bbox[0]*(shape[1]/448) ) , int( bbox[1]*(shape[1]/448) ) , #x1 x2\n",
    "                 int( bbox[2]*(shape[0]/448) ) , int( bbox[3]*(shape[0]/448) ) ] #y1 y2\n",
    "    \n",
    "    def _confidence_p(self , output):\n",
    "        #置信度与分类概率的乘积\n",
    "        prod = []\n",
    "        \n",
    "        for i in range(7):\n",
    "            for j in range(7):\n",
    "                prod.append(output[i,j,0] * output[i,j, 10:]) #bnd 1\n",
    "                prod.append(output[i,j,5] * output[i,j, 10:]) #bnd 2\n",
    "                \n",
    "        prod = np.array(prod) #98*20\n",
    "        prod = prod.T #20*98\n",
    "        \n",
    "        return prod            \n",
    "    \n",
    "    def _bbox(self , output):\n",
    "        #将output中的坐标还原为448*448坐标系中\n",
    "        def __bbox(bnd , grid):\n",
    "            target_x = bnd[0]\n",
    "            target_y = bnd[1]\n",
    "            target_w = bnd[2]\n",
    "            target_h = bnd[3]\n",
    "            \n",
    "            center_x = int( target_x * 64 + grid[0] )\n",
    "            center_y = int( target_y * 64 + grid[2] )\n",
    "            \n",
    "            w = int( target_w * 448 )\n",
    "            h = int( target_h * 448 )\n",
    "            \n",
    "            x1 = int( center_x - 0.5*w )\n",
    "            x2 = int( center_x + 0.5*w )\n",
    "            y1 = int( center_y - 0.5*h )\n",
    "            y2 = int( center_y + 0.5*h )\n",
    "            \n",
    "            return np.clip([x1,x2,y1,y2] , a_min=0 , a_max=448) #对于跨越边界的框进行clip\n",
    "        \n",
    "        bbox = []\n",
    "        \n",
    "        for i in range(7):\n",
    "            for j in range(7):\n",
    "                bbox.append( __bbox(output[i,j,1:5] , self.grid[i][j]) ) #bnd 1\n",
    "                bbox.append( __bbox(output[i,j,6:10], self.grid[i][j]) ) #bnd 2\n",
    "    \n",
    "        bbox = np.array(bbox) #98*4\n",
    "        bbox = bbox.T #4*98\n",
    "        \n",
    "        return bbox\n",
    "        \n",
    "\n",
    "    def _nms(self , prod , bbox , des_idx , threshold_nms):\n",
    "        '''\n",
    "        prod已降序排列 bbox也对应降序 des_idx为在未降序的数组中的 降序索引(argsort)\n",
    "        '''\n",
    "        length = len(prod) #98\n",
    "        lost_flag = [1]*length #标记丢弃的框 0表示丢弃\n",
    "        \n",
    "        max_score_idx = 0 #记录当前最大score的idx\n",
    "        \n",
    "        #对于score为0的bnd box不必进行nms 直接丢弃\n",
    "        for i in range(length):\n",
    "            if prod[i] == 0.0:\n",
    "                lost_flag[i:] = [0] * (length - i) #因为prod已经降序 故出现0.0 后续全为0.0\n",
    "                break\n",
    "        \n",
    "        while max_score_idx < length:\n",
    "            max_score_rect = bbox[: , max_score_idx]\n",
    "            \n",
    "            for i in range(max_score_idx+1 , length):\n",
    "                if lost_flag[i] == 1 and self._iou( max_score_rect , bbox[: , i] ) > threshold_nms: #大于阈值 丢弃\n",
    "                    lost_flag[i] = 0\n",
    "\n",
    "            max_score_idx_bak = max_score_idx #后续使用\n",
    "            \n",
    "            #让max_score_idx指向下一个没被丢弃的最大值\n",
    "            for i in range(max_score_idx+1 , length):\n",
    "                if lost_flag[i] == 1:\n",
    "                    max_score_idx = i\n",
    "                    break\n",
    "            \n",
    "            #说明max_score_idx没有移动过 即后续的都被丢弃了 提前终止循环\n",
    "            if max_score_idx == max_score_idx_bak:\n",
    "                break\n",
    "            \n",
    "        nms_idx = [] #用来存放丢弃的bnd box!!!\n",
    "\n",
    "        for i in range(length):\n",
    "            if lost_flag[i] == 0:\n",
    "                nms_idx.append( des_idx[i] )\n",
    "                \n",
    "        return nms_idx\n",
    "    \n",
    "    \n",
    "    def _iou(self , bbox_a , bbox_b):\n",
    "        #计算bbox面积\n",
    "        def __area(bbox):\n",
    "            w = bbox[1] - bbox[0]\n",
    "            h = bbox[3] - bbox[2]\n",
    "\n",
    "            return w*h\n",
    "    \n",
    "        xmin_a = bbox_a[0]\n",
    "        xmax_a = bbox_a[1]\n",
    "        ymin_a = bbox_a[2]\n",
    "        ymax_a = bbox_a[3]\n",
    "        \n",
    "        xmin_b = bbox_b[0]\n",
    "        xmax_b = bbox_b[1]\n",
    "        ymin_b = bbox_b[2]\n",
    "        ymax_b = bbox_b[3]\n",
    "        \n",
    "        if   xmin_a < xmax_b <= xmax_a and (ymin_a < ymax_b <= ymax_a or ymin_a <= ymin_b < ymax_a):\n",
    "            flag = True\n",
    "        elif xmin_a <= xmin_b < xmax_a and (ymin_a < ymax_b <= ymax_a or ymin_a <= ymin_b < ymax_a):\n",
    "            flag = True\n",
    "        elif xmin_b < xmax_a <= xmax_b and (ymin_b < ymax_a <= ymax_b or ymin_b <= ymin_a < ymax_b):\n",
    "            flag = True\n",
    "        elif xmin_b <= xmin_a < xmax_b and (ymin_b < ymax_a <= ymax_b or ymin_b <= ymin_a < ymax_b):\n",
    "            flag = True\n",
    "        else:\n",
    "            flag = False\n",
    "        \n",
    "        if flag:\n",
    "            x_sorted_list = sorted([xmin_a, xmax_a, xmin_b, xmax_b])\n",
    "            y_sorted_list = sorted([ymin_a, ymax_a, ymin_b, ymax_b])\n",
    "            \n",
    "            x_intersect_w = x_sorted_list[2] - x_sorted_list[1] #0 1 2 3\n",
    "            y_intersect_h = y_sorted_list[2] - y_sorted_list[1] #0 1 2 3\n",
    "            \n",
    "            area_inter = x_intersect_w * y_intersect_h #计算重合面积\n",
    "            \n",
    "            union_area = __area(bbox_a) + __area(bbox_b) - area_inter\n",
    "            \n",
    "            return area_inter/union_area\n",
    "        else:\n",
    "            return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = YOLO_V2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from save/model/model.ckpt-290\n",
      "0 11.899965\n",
      "10 2.6669745\n",
      "20 19.913555\n",
      "30 11.7010765\n",
      "40 3.1835704\n",
      "50 29.869507\n",
      "60 3.508064\n",
      "70 5.7225046\n",
      "80 3.1015394\n",
      "90 4.790275\n",
      "100 5.5282784\n",
      "110 3.9910197\n",
      "120 27.844131\n",
      "130 5.255406\n",
      "140 2.4926865\n",
      "150 3.6026378\n",
      "160 6.209631\n",
      "170 18.191385\n",
      "180 4.6523376\n",
      "190 3.5999625\n",
      "200 4.7379174\n",
      "210 5.1759095\n",
      "220 2.6724927\n",
      "230 11.298651\n",
      "240 6.9037914\n",
      "250 3.919413\n",
      "260 9.728306\n",
      "270 3.6226234\n",
      "280 5.5427094\n",
      "290 9.907307\n"
     ]
    }
   ],
   "source": [
    "test.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testt = YOLO_V2(is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from save/model/model.ckpt-290\n",
      "test image: ../../../tensorflow2/dataset/VOC2012test/JPEGImages\\2008_007489.jpg\n",
      "debug: (375, 500, 3) [10] [[95, 233, 178, 281]]\n"
     ]
    }
   ],
   "source": [
    "testt.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from save/model/model.ckpt-290\n",
      "test image: ../../../tensorflow2/dataset/VOC2012test/JPEGImages\\2010_006259.jpg\n",
      "debug: (375, 500, 3) [5, 14, 3, 19, 10, 10, 15] [[-2, 17, 0, 129], [285, 304, 99, 114], [0, 0, 160, 160], [81, 261, 174, 285], [212, 372, 172, 272], [285, 285, 207, 220], [59, 225, 321, 321]]\n"
     ]
    }
   ],
   "source": [
    "testt.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.00819682,\n",
       "        0.        , 0.        ],\n",
       "       [0.06861994, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.0195321 ],\n",
       "       [0.01040532, 0.        , 0.        , 0.        , 0.00409173,\n",
       "        0.        , 0.        ],\n",
       "       [0.06733071, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.02720767, 0.        ],\n",
       "       [0.        , 0.        , 0.12342051, 0.        , 0.0597523 ,\n",
       "        0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testt._output[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.01564434,\n",
       "        0.        , 0.        ],\n",
       "       [0.07016775, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.02026871],\n",
       "       [0.00974074, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        ],\n",
       "       [0.07869903, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.02900382, 0.        ],\n",
       "       [0.        , 0.        , 0.13965729, 0.        , 0.07298322,\n",
       "        0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testt._output[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
