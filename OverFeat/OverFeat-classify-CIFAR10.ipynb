{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import cv2\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import xml\n",
    "from xml.etree import ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LABEL2INDEX = {'frog':0 , 'truck':1 , 'deer':2 , 'automobile':3 , 'bird':4 , 'horse':5 , 'ship':6 , 'cat':7 , 'dog':8 , 'airplane':9}\n",
    "INDEX2LABEL = {value:key for key , value in LABEL2INDEX.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#调整尺寸\n",
    "def resize(img , resize_type):\n",
    "    if resize_type == 'reg':\n",
    "        #resize_val 为(x,x)\n",
    "        return tf.image.resize_images(img , (256,256))\n",
    "    \n",
    "    elif resize_type == 'min':\n",
    "        #resize_val scalar\n",
    "        height = tf.cast(img.shape[0] , dtype=tf.float32)\n",
    "        width = tf.cast(img.shape[1] , dtype=tf.float32)\n",
    "        ratio = tf.divide(height , width)\n",
    "        \n",
    "        new_shape = tf.cond(tf.less(height, width), lambda: (tf.constant(256, dtype=tf.int32),\n",
    "                                                           tf.cast(tf.floor(tf.divide(256.0, ratio)) , dtype=tf.int32)),\n",
    "                     lambda: (\n",
    "                         tf.cast(tf.floor(tf.multiply(256.0 , ratio)), dtype=tf.int32),\n",
    "                         tf.constant(256, dtype=tf.int32)))\n",
    "        \n",
    "        return tf.image.resize_images(img , new_shape)\n",
    "    \n",
    "    elif resize_type == 'max':\n",
    "        #resize_val scalar\n",
    "        height = tf.cast(img.shape[0] , dtype=tf.float32)\n",
    "        width = tf.cast(img.shape[1] , dtype=tf.float32)\n",
    "        ratio = tf.divide(height , width)\n",
    "        \n",
    "        new_shape = tf.cond(tf.less(height, width),lambda: (\n",
    "                         tf.cast(tf.floor(tf.multiply(256.0 , ratio)), dtype=tf.int32),\n",
    "                         tf.constant(256, dtype=tf.int32)), \n",
    "                            lambda: (tf.constant(256, dtype=tf.int32),\n",
    "                                                           tf.cast(tf.floor(tf.divide(256.0, ratio)) , dtype=tf.int32)))\n",
    "        \n",
    "        return tf.image.resize_images(img , new_shape)\n",
    "    \n",
    "#随机裁剪\n",
    "def random_clip(img , crop_size):\n",
    "    #shape = img.shape\n",
    "    #\n",
    "    #if shape[0] == crop_size[0] and shape[1] == crop_size[1]:\n",
    "    #    return img\n",
    "    #\n",
    "    #height_clip_domain = shape[0]-crop_size[0]\n",
    "    #width_clip_domain = shape[1]-crop_size[1]\n",
    "    #\n",
    "    #height_clip_idx = np.random.randint(0 , height_clip_domain)\n",
    "    #width_clip_idx = np.random.randint(0 , width_clip_domain)\n",
    "    #\n",
    "    #return img[height_clip_idx:height_clip_idx+crop_size[0] , width_clip_idx:width_clip_idx+crop_size[1] , :]\n",
    "\n",
    "    return tf.random_crop(img , crop_size)\n",
    "    \n",
    "\n",
    "def central_clip(img , crop_size):\n",
    "    img_height = img.shape[0]\n",
    "    img_width = img.shape[1]\n",
    "    \n",
    "    height_r = img_height - crop_size[0]\n",
    "    width_r = img_width - crop_size[1]\n",
    "    \n",
    "    top = height_r//2\n",
    "    left = width_r//2\n",
    "    \n",
    "    img = tf.slice(img , [top , left , 0] , [crop_size[0] , crop_size[1] , 3])\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(train_size = 0.8):\n",
    "    filenames = os.listdir(path='../../tensorflow2/dataset/CIFAR10/train/')\n",
    "    filenames = sorted(filenames , key=lambda filename: int(filename.split('.')[0] )) #与labels顺序对应\n",
    "    filenames = np.array(filenames)\n",
    "    \n",
    "    labels = pd.read_csv('../../tensorflow2/dataset/CIFAR10/trainLabels.csv')['label'].map(LABEL2INDEX).get_values()\n",
    "    \n",
    "    idx = list(range(len(filenames)))\n",
    "    np.random.shuffle(idx)    \n",
    "    \n",
    "    train_idx = idx[ : int(len(idx) * train_size)]\n",
    "    val_idx = idx[int(len(idx) * train_size) : ]\n",
    "    \n",
    "    return filenames[train_idx] , labels[train_idx] , filenames[val_idx] , labels[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FAST = True\n",
    "ACCURATE = not FAST\n",
    "\n",
    "if FAST:\n",
    "    HEIGHT = 231\n",
    "    WIDTH = 231\n",
    "else:\n",
    "    HEIGHT = 221\n",
    "    WIDTH = 221"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_train(filename , label):\n",
    "    #训练数据预处理\n",
    "    img = tf.read_file('../../tensorflow2/dataset/CIFAR10/train/' + filename)\n",
    "    img = tf.image.decode_image(img)\n",
    "    img = tf.cast(img , tf.float32)\n",
    "    img = tf.reshape(img, tf.stack([32, 32, 3])) #关键操作\n",
    "    \n",
    "    img = resize(img , 'max')\n",
    "    img = random_clip(img , crop_size=(HEIGHT , WIDTH , 3))\n",
    "    \n",
    "    # img = tf.random_crop(img , size = [HEIGHT , WIDTH , 3])\n",
    "    # img = tf.image.random_flip_left_right(img)\n",
    "    \n",
    "    #img = cv2.imread('../../tensorflow2/dataset/CIFAR10/train/' + filename)\n",
    "    \n",
    "    \n",
    "    img = img/127.5 - 1.0\n",
    "    \n",
    "    return img , label\n",
    "    \n",
    "\n",
    "def preprocess_val(filename , label):\n",
    "    #验证数据预处理\n",
    "    img = tf.read_file('../../tensorflow2/dataset/CIFAR10/train/' + filename)\n",
    "    img = tf.image.decode_image(img)\n",
    "    img = tf.cast(img , tf.float32)\n",
    "    img = tf.reshape(img, tf.stack([32, 32, 3])) #关键操作\n",
    "    \n",
    "    img = resize(img , 'max')\n",
    "    img = random_clip(img , crop_size=(HEIGHT , WIDTH , 3))\n",
    "    \n",
    "    # img = tf.random_crop(img , size = [HEIGHT , WIDTH , 3])\n",
    "    # img = tf.image.random_flip_left_right(img)\n",
    "    \n",
    "    img = img/127.5 - 1.0\n",
    "    \n",
    "    # img = central_clip(img , crop_size=(HEIGHT , WIDTH))\n",
    "    \n",
    "    return img , label\n",
    "\n",
    "def build_dataset(filenames , labels , is_training = True , batch_size=128):\n",
    "    dataset = tf.data.Dataset()\n",
    "    dataset = dataset.from_tensor_slices(( filenames , labels )) #每个元素为(filename label) #传入xml路径信息\n",
    "\n",
    "    if is_training:\n",
    "        dataset = dataset.map(preprocess_train)\n",
    "        #dataset = dataset.map(lambda filename , label : tuple( tf.py_func( preprocess_train , [filename , label] , [tf.float32 , tf.int16] )) )\n",
    "    else:\n",
    "        dataset = dataset.map(preprocess_val)\n",
    "    \n",
    "    dataset = dataset.shuffle(buffer_size=1000).repeat().batch(batch_size)\n",
    "    \n",
    "    iterator = dataset.make_initializable_iterator()\n",
    "    \n",
    "    return  iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_filenames , train_labels , val_filenames , val_labels = split_data(train_size=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "iterator_train = build_dataset( train_filenames , train_labels ) #传入路径\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "iterator_val = build_dataset(val_filenames , val_labels , is_training=False) #传入路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class OverFeat(object):\n",
    "\n",
    "    def __init__(self , iterator_train , iterator_val , num_classes , model_type = 'fast'):\n",
    "\n",
    "        #需要对其initializer进行初始化 sess.run(xxx.initializer)\n",
    "        self.iterator_train = iterator_train\n",
    "        self.iterator_val = iterator_val\n",
    "\n",
    "        self.IS_TRAINING = tf.placeholder(dtype=tf.bool)\n",
    "        \n",
    "        self.X = iterator_train.get_next()[0]\n",
    "        self.y = iterator_train.get_next()[1]\n",
    "        \n",
    "        #self.X = tf.cond( tf.equal(self.IS_TRAINING , tf.constant(True)) , lambda : iterator_train.get_next()[0] , lambda : iterator_val.get_next()[0] )\n",
    "        #self.y = tf.cond( tf.equal(self.IS_TRAINING , tf.constant(True)) , lambda : iterator_train.get_next()[1] , lambda : iterator_val.get_next()[1] )\n",
    "        \n",
    "        self.NUM_CLASSES = num_classes\n",
    "        self.TYPE = model_type\n",
    "        \n",
    "        self.EPOCH = 90 #paper\n",
    "        \n",
    "        self.BATCH_SIZE = 128 #paper\n",
    "        \n",
    "        self.KEEP_PROB = 0.5\n",
    "        \n",
    "        if model_type == 'accurate':\n",
    "            #self.HEIGHT = 221\n",
    "            #self.WIDTH = 221\n",
    "            \n",
    "            self.model_accurate()\n",
    "        else:\n",
    "            #self.HEIGHT = 231\n",
    "            #self.WIDTH = 231\n",
    "            \n",
    "            self.model_fast()\n",
    "\n",
    "    def model_fast(self):\n",
    "        conv1 = self.conv(self.X , 11 , 11 , 96 , 4 , 4 , name='conv1')\n",
    "        max_pooling1 = self.max_pooling(conv1 , 2 , 2 , 2 , 2 , name='pooling1')\n",
    "        \n",
    "        conv2 = self.conv(max_pooling1 , 5 , 5 , 256 , 1 , 1 , name='conv2')\n",
    "        max_pooling2 = self.max_pooling(conv2 , 2 , 2 , 2 , 2 , name='pooling2')\n",
    "        \n",
    "        conv3 = self.conv(max_pooling2 , 3 , 3 , 512 , 1 , 1 , name='conv3')\n",
    "        \n",
    "        conv4 = self.conv(conv3 , 3 , 3 , 1024 , 1 , 1 , name='conv4')\n",
    "    \n",
    "        conv5 = self.conv(conv4 , 3 , 3 , 1024 , 1 , 1 , name='conv5')\n",
    "        max_pooling5 = self.max_pooling(conv5 , 2 , 2 , 2 , 2 , name='pooling5')\n",
    "        \n",
    "        #conv6 = self.fcn(max_pooling5 , output_channel=3072 , name='fcn') #FCN形式 全连接变为卷积形式\n",
    "        #===\n",
    "        max_pooling5 = tf.layers.flatten(max_pooling5)\n",
    "        #===\n",
    "        \n",
    "        fc6 = self.fc(max_pooling5 , 3072 , name='fc6')\n",
    "        fc6 = tf.layers.dropout(fc6 , rate=1. - self.KEEP_PROB)\n",
    "            \n",
    "        fc7 = self.fc(fc6 , 4096 , name='fc7')\n",
    "        fc7 = tf.layers.dropout(fc7 , rate=1. - self.KEEP_PROB)\n",
    "        \n",
    "        fc8 = self.fc(fc7 , self.NUM_CLASSES , name='fc8')\n",
    "        \n",
    "        self.logits = fc8\n",
    "        \n",
    "    \n",
    "    def model_accurate(self):\n",
    "        conv1 = self.conv(self.X , 7 , 7 , 96 , 2 , 2 , name='conv1')\n",
    "        max_pooling1 = self.max_pooling(conv1 , 3 , 3 , 3 , 3 , name='pooling1')\n",
    "        \n",
    "        conv2 = self.conv(max_pooling1 , 7 , 7 , 256 , 1 , 1 , name='conv2')\n",
    "        max_pooling2 = self.max_pooling(conv2 , 2 , 2 , 2 , 2 , name='pooling2')\n",
    "        \n",
    "        conv3 = self.conv(max_pooling2 , 3 , 3 , 512 , 1 , 1 , name='conv3')\n",
    "        \n",
    "        conv4 = self.conv(conv3 , 3 , 3 , 512 , 1 , 1 , name='conv4')\n",
    "        \n",
    "        conv5 = self.conv(conv4 , 3 , 3 , 1024 , 1 , 1 , name='conv5')\n",
    "        \n",
    "        conv6 = self.conv(conv5 , 3 , 3 , 1024 , 1 , 1 , name='conv6')\n",
    "        max_pooling6 = self.max_pooling(conv6 , 3 , 3 , 3 , 3 , name='pooling6')\n",
    "        \n",
    "        #conv7 = self.fcn(max_pooling6 , output_channel = 4096 , name='fcn') #FCN形式 全连接变为卷积形式\n",
    "        #===\n",
    "        max_pooling6 = tf.layers.flatten(max_pooling6)\n",
    "        #===\n",
    "        \n",
    "        fc7 = self.fc(max_pooling6 , 4096 , name='fc7')\n",
    "        fc7 = tf.layers.dropout(fc7 , rate=1. - self.KEEP_PROB)\n",
    "        \n",
    "        fc8 = self.fc(fc7 , 4096 , name='fc8')\n",
    "        fc8 = tf.layers.dropout(fc8 , rate=1. - self.KEEP_PROB)\n",
    "                \n",
    "        fc9 = self.fc(fc8 , self.NUM_CLASSES , name='fc9')\n",
    "        \n",
    "        self.logits = fc9\n",
    "    \n",
    "    def train(self):\n",
    "        #验证使用\n",
    "        predictions = tf.nn.softmax(self.logits)\n",
    "        predictions = tf.argmax(predictions , axis=-1)\n",
    "        equal = tf.equal(predictions , self.y) #bool\n",
    "        equal = tf.cast(equal , dtype=tf.int32)\n",
    "        accu = tf.reduce_sum(equal)/self.BATCH_SIZE\n",
    "        \n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.y , logits=self.logits)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        #构建训练过程\n",
    "        \n",
    "        epoch = tf.Variable(initial_value=0 , name='epoch' , trainable=False)\n",
    "        epoch_add = tf.assign_add(epoch , value=1) #对epoch加1 因为下面的lr需要变化\n",
    "        \n",
    "        learning_rate = tf.train.piecewise_constant(epoch , boundaries=[30,50,60,70,80] ,\n",
    "                                                    values=[0.05,0.025,0.0125,0.00625,0.003125,0.0015625])\n",
    "        optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate , momentum=0.6)\n",
    "        \n",
    "        train_op = optimizer.minimize(loss)\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            \n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "            for i in range(self.EPOCH):\n",
    "                sess.run(self.iterator_train.initializer)\n",
    "                \n",
    "                while True:\n",
    "                    try:\n",
    "                        _ , _loss = sess.run((train_op , loss) , feed_dict={self.IS_TRAINING : True})\n",
    "                        \n",
    "                        print(_loss)\n",
    "                        \n",
    "                    except tf.errors.OutOfRangeError:\n",
    "                        #一个epoch训练完毕\n",
    "                        #可以进行验证 为验证做准备\n",
    "\n",
    "                        #运行验证数据集迭代器\n",
    "                        sess.run(self.iterator_val.initializer)\n",
    "\n",
    "                        while True:\n",
    "                            try:\n",
    "                                _accu = sess.run(accu , feed_dict={self.IS_TRAINING : False})\n",
    "\n",
    "                                print(_accu)\n",
    "\n",
    "                            except tf.errors.OutOfRangeError:\n",
    "                                #验证完毕 继续训练\n",
    "                                sess.run(epoch_add)\n",
    "\n",
    "                                #跳出内循环（验证循环）\n",
    "                                break\n",
    "                    \n",
    "                        #跳出外循环（训练循环）\n",
    "                        break\n",
    "                \n",
    "        \n",
    "    \n",
    "    def predict(self):\n",
    "        pass\n",
    "    \n",
    "    def conv(self , x , filter_height , filter_width , output_channel , stride_height , stride_width , name , padding='same'):\n",
    "        \n",
    "        return tf.layers.conv2d(x , output_channel , [filter_height , filter_width] , [stride_height , stride_width] , padding=padding ,\n",
    "                             activation=tf.nn.relu , kernel_initializer = tf.initializers.random_normal(stddev=1e-2) ,\n",
    "                             bias_initializer = tf.initializers.constant() , kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=1e-5) ,\n",
    "                             name=name , reuse=tf.AUTO_REUSE)\n",
    "    \n",
    "        #input_channel = x.get_shape().as_list()[-1]\n",
    "        \n",
    "        #with tf.variable_scope(name) as scope:\n",
    "            #weights = tf.get_variable(name='weights' , shape=[filter_height , filter_width , input_channel , output_channel] , initializer=tf.random_normal_initializer(0.0 , 1e-2) , regularizer=tf.contrib.layers.l2_regularizer(scale=1e-5))\n",
    "            #biases = tf.get_variable(name='biases' , shape=[output_channel] , initializer=tf.constant_initializer())\n",
    "            #\n",
    "            #conv = tf.nn.conv2d(x , weights , strides=[1 , stride_height , stride_width , 1] , padding=padding)\n",
    "            #biases = tf.nn.bias_add(conv , biases)\n",
    "            #\n",
    "            #relu = tf.nn.relu(biases)\n",
    "            \n",
    "            #return relu\n",
    "            \n",
    "    #3*3 pooling\n",
    "    def max_pooling(self , x , pooling_height , pooling_width , stride_height , stride_width  , name , padding='same'):\n",
    "        #return tf.nn.max_pool(x , [1 , pooling_height , pooling_width , 1] , strides=[1 , stride_height , stride_width , 1] , padding=padding , name=name)\n",
    "        \n",
    "        return tf.layers.max_pooling2d(x , [pooling_height , pooling_width] , [stride_height , stride_width] , padding=padding , name=name)\n",
    "    \n",
    "    def fc(self , x , output_size , name):\n",
    "        \n",
    "        return tf.layers.dense(x , output_size , activation=tf.nn.relu , kernel_initializer=tf.initializers.random_normal(stddev=1e-2) ,\n",
    "                               bias_initializer = tf.initializers.constant() , kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=1e-5),\n",
    "                               name = name , reuse=tf.AUTO_REUSE)\n",
    "        \n",
    "        #with tf.variable_scope(name) as scope:\n",
    "        #    weights = tf.get_variable(name='weights' , shape=[input_size , output_size] , initializer=tf.random_normal_initializer())\n",
    "        #    biases = tf.get_variable(name='biases' , shape=[output_size] , initializer=tf.constant_initializer())\n",
    "        #    \n",
    "        #    biases = tf.nn.bias_add(tf.matmul(x , weights) , biases)\n",
    "        #    \n",
    "        #    if relu:\n",
    "        #        return tf.nn.relu(biases)\n",
    "        #    else:\n",
    "        #        return biases\n",
    "        #\n",
    "    \n",
    "    #所有卷积层后紧跟的fc层变为卷积层方式\n",
    "    def fcn(self , x , output_channel , name , padding='same'):\n",
    "        if self.TYPE == 'accurate':\n",
    "            return tf.layers.conv2d(x , output_channel , [6 , 6] , [1 , 1] , padding=padding ,\n",
    "                             activation=tf.nn.relu , kernel_initializer = tf.initializers.random_normal(stddev=1e-2) ,\n",
    "                             bias_initializer = tf.initializers.constant() , kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=1e-5) ,\n",
    "                             name=name , reuse=tf.AUTO_REUSE)\n",
    "        else:\n",
    "            return tf.layers.conv2d(x , output_channel , [5 , 5] , [1 , 1] , padding=padding ,\n",
    "                             activation=tf.nn.relu , kernel_initializer = tf.initializers.random_normal(stddev=1e-2) ,\n",
    "                             bias_initializer = tf.initializers.constant() , kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=1e-5) ,\n",
    "                             name=name , reuse=tf.AUTO_REUSE)\n",
    "        \n",
    "    def batch_norm(self , x , name):\n",
    "        return tf.layers.batch_normalization(x , axis=-1 , training=self.IS_TRAINING , renorm=True , fused=True , name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''model fast'''\n",
    "overfeat = OverFeat(iterator_train , iterator_val , 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(231), Dimension(231), Dimension(3)])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overfeat.X.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "overfeat.train() #传入训练和验证的迭代器 需要进行sess.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
