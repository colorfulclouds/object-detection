{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import scipy.io as sio\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = list()\n",
    "\n",
    "for img in os.listdir('../../tensorflow2/dataset/102flowers/jpg/'):\n",
    "    data_dir.append(os.path.join('../../tensorflow2/dataset/102flowers/jpg/' , img))\n",
    "\n",
    "data_dir.sort()\n",
    "\n",
    "data_dir = np.array(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = sio.loadmat('../../tensorflow2/dataset/imagelabels.mat')\n",
    "\n",
    "setid = sio.loadmat('../../tensorflow2/dataset/setid.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = labels['labels'][0]-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trnid = np.array(setid['tstid'][0]) - 1 #train index\n",
    "tstid = np.array(setid['trnid'][0]) - 1 #test index\n",
    "valid = np.array(setid['valid'][0]) - 1 #val index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_classes = 102 #102种花"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_num = len(trnid)\n",
    "test_num = len(tstid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_batch(batch_size = 128 , is_training = True):\n",
    "    data = []\n",
    "    _labels = []\n",
    "    \n",
    "    if is_training:\n",
    "        #训练数据随机索引\n",
    "        shuffle_idx = np.random.randint(low=0 , high=train_num , size=batch_size)\n",
    "        \n",
    "        for i in trnid[shuffle_idx]:\n",
    "            img = cv2.imread(data_dir[i])\n",
    "            img = cv2.resize(img , (224,224))\n",
    "            img = img/127.5-1.0\n",
    "            \n",
    "            label = labels[i]\n",
    "            \n",
    "            data.append(img)\n",
    "            _labels.append(label)\n",
    "            \n",
    "        return np.array(data) , to_categorical(np.array(_labels) , num_classes=num_classes)\n",
    "            \n",
    "    else:\n",
    "        #验证数据随机索引\n",
    "        shuffle_idx = np.random.randint(low=0 , high=test_num , size=batch_size)\n",
    "        \n",
    "        for i in tstid[shuffle_idx]:\n",
    "            img = cv2.imread(data_dir[i])\n",
    "            img = cv2.resize(img , (224,224))\n",
    "            img = img/127.5-1.0\n",
    "            \n",
    "            label = labels[i]\n",
    "            \n",
    "            data.append(img)\n",
    "            _labels.append(label)\n",
    "            \n",
    "        return np.array(data) , to_categorical(np.array(_labels) , num_classes=num_classes)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def spatial_pyramid_pool(conv5 , pyramid_bins):\n",
    "    batch_size = conv5.get_shape().as_list()[0] #batch_size\n",
    "    conv5_height = conv5.get_shape().as_list()[1] #feature map height\n",
    "    conv5_width = conv5.get_shape().as_list()[2] #feature map width\n",
    "\n",
    "    for i in range(len(pyramid_bins)):\n",
    "        pooling_height = np.ceil(conv5_height / pyramid_bins[i])\n",
    "        stride_height = np.ceil(conv5_height / pyramid_bins[i]) #floor\n",
    "        \n",
    "        pooling_width = np.ceil(conv5_width / pyramid_bins[i])\n",
    "        stride_width = np.ceil(conv5_width / pyramid_bins[i]) #floor\n",
    "        \n",
    "        padding_height = int(pyramid_bins[i] * pooling_height - conv5_height)\n",
    "        padding_width = int(pyramid_bins[i] * pooling_width - conv5_width)\n",
    "        \n",
    "        conv5_padding = tf.pad(conv5 , tf.constant([[0,0] , [0,padding_height] , [0,padding_width] ,[0,0]]))\n",
    "        \n",
    "        #max_pooling = tf.layers.max_pooling2d(conv5_padding , [pooling_height , pooling_width] , [stride_height , stride_width] , padding='same')\n",
    "        max_pooling = tf.nn.max_pool(conv5_padding , ksize=[1,pooling_height,pooling_width,1] , strides=[1,stride_height,stride_width,1] , padding='SAME')\n",
    "        \n",
    "        if i==0:\n",
    "            spp = tf.reshape(max_pooling , shape=(batch_size , -1))\n",
    "        else:\n",
    "            spp = tf.concat(values=[spp , tf.reshape(max_pooling , shape=(batch_size , -1)) ] , axis=-1)\n",
    "            \n",
    "    return spp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#获取预训练参数\n",
    "net_data = np.load('bvlc_alexnet.npy' , encoding='bytes').item() #不加encoding='bytes' 死机\n",
    "#爆内存 不使用\n",
    "conv1w = tf.Variable(net_data[\"conv1\"][0] , trainable=False) #11*11\n",
    "conv1b = tf.Variable(net_data[\"conv1\"][1] , trainable=False)\n",
    "\n",
    "conv2w = tf.Variable(net_data[\"conv2\"][0] , trainable=False) #5*5\n",
    "conv2b = tf.Variable(net_data[\"conv2\"][1] , trainable=False)\n",
    "\n",
    "conv3w = tf.Variable(net_data[\"conv3\"][0] , trainable=False) #3*3\n",
    "conv3b = tf.Variable(net_data[\"conv3\"][1] , trainable=False)\n",
    "\n",
    "conv4w = tf.Variable(net_data[\"conv4\"][0] , trainable=False) #3*3\n",
    "conv4b = tf.Variable(net_data[\"conv4\"][1] , trainable=False)\n",
    "\n",
    "conv5w = tf.Variable(net_data[\"conv5\"][0] , trainable=False) #3*3\n",
    "conv5b = tf.Variable(net_data[\"conv5\"][1] , trainable=False)\n",
    "\n",
    "fc6w = tf.Variable(tf.truncated_normal(shape=((8**2+6**2+4**2)*256 , 1024) , stddev=1e-2))\n",
    "fc6b = tf.Variable(tf.constant(0.1 , shape=[1024]))\n",
    "#fc6b = tf.Variable(net_data['fc6'][1])\n",
    "\n",
    "fc7w = tf.Variable(tf.truncated_normal(shape=(1024 , num_classes) , stddev=1e-2))\n",
    "fc7b = tf.Variable(tf.constant(0.1 , shape=[num_classes]))\n",
    "\n",
    "#下面不敢使用 爆显存\n",
    "#fc7w = tf.Variable(net_data['fc7'][0])\n",
    "#fc7b = tf.Variable(net_data['fc7'][1])\n",
    "\n",
    "#fc8w = tf.Variable(tf.truncated_normal(shape=(4096 , num_classes) , stddev=1e-2))\n",
    "#fc8b = tf.Variable(tf.constant(0.1 , shape=[num_classes]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def group_conv(x , kernel , strides):\n",
    "    group_x = tf.split(x , num_or_size_splits=2 , axis=3)\n",
    "    group_kernel = tf.split(kernel , num_or_size_splits=2 , axis=3)\n",
    "    \n",
    "    group_conv0 = tf.nn.conv2d(group_x[0] , group_kernel[0] , strides=strides , padding='SAME')\n",
    "    group_conv1 = tf.nn.conv2d(group_x[1] , group_kernel[1] , strides=strides , padding='SAME')\n",
    "    \n",
    "    group_conv = tf.concat((group_conv0 , group_conv1) , axis=3)\n",
    "    \n",
    "    return group_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep_prob = tf.placeholder(dtype=tf.float32)\n",
    "pyramid_bins = [8,6,4] #spp中使用3种格子\n",
    "\n",
    "def model_finetune(x):\n",
    "    conv1 = tf.nn.conv2d(x , conv1w , strides=(1,4,4,1) , padding='SAME')\n",
    "    conv1 = tf.nn.bias_add(conv1 , conv1b)\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "    lrn1 = tf.nn.local_response_normalization(conv1 , depth_radius=5 , alpha=0.0001 , beta=0.75 , bias=1.0)\n",
    "    maxpool1 = tf.nn.max_pool(lrn1 , ksize=(1,3,3,1) , strides=(1,2,2,1) , padding='VALID')\n",
    "    \n",
    "    conv2 = group_conv(maxpool1 , conv2w , strides=(1,1,1,1))\n",
    "    conv2 = tf.nn.bias_add(conv2 , conv2b)\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "    lrn2 = tf.nn.local_response_normalization(conv2 , depth_radius=5 , alpha=0.0001 , beta=0.75 , bias=1.0)\n",
    "    maxpool2 = tf.nn.max_pool(lrn2 , ksize=(1,3,3,1) , strides=(1,2,2,1) , padding='VALID')\n",
    "    \n",
    "    conv3 = tf.nn.conv2d(maxpool2 , conv3w , strides=(1,1,1,1) , padding='SAME')\n",
    "    conv3 = tf.nn.bias_add(conv3 , conv3b)\n",
    "    conv3 = tf.nn.relu(conv3)\n",
    "\n",
    "    conv4 = group_conv(conv3 , conv4w , strides=(1,1,1,1))\n",
    "    conv4 = tf.nn.bias_add(conv4 , conv4b)\n",
    "    conv4 = tf.nn.relu(conv4)\n",
    "\n",
    "    conv5 = group_conv(conv4 , conv5w , strides=(1,1,1,1))\n",
    "    conv5 = tf.nn.bias_add(conv5 , conv5b)\n",
    "    conv5 = tf.nn.relu(conv5)\n",
    "\n",
    "    maxpool5 = spatial_pyramid_pool(conv5 , pyramid_bins)\n",
    "    \n",
    "    # print(maxpool5.get_shape().as_list()) #debug\n",
    "    \n",
    "    fc6 = tf.nn.relu_layer(maxpool5 , fc6w , fc6b)\n",
    "    fc6 = tf.nn.dropout(fc6 , keep_prob)\n",
    "    \n",
    "    fc7 = tf.nn.xw_plus_b(fc6 , fc7w , fc7b)\n",
    "    \n",
    "    return fc7\n",
    "    \n",
    "    #下面的不敢使用 爆显存\n",
    "    #fc7 = tf.nn.relu_layer(fc6 , fc7w , fc7b)\n",
    "    #fc7 = tf.nn.dropout(fc7 , keep_prob)\n",
    "    #\n",
    "    #fc8 = tf.nn.xw_plus_b(fc7 , fc8w , fc8b) #需要softmax激活\n",
    "    \n",
    "    #return fc8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(x):\n",
    "    conv1 = tf.layers.conv2d(x , filters=96 , kernel_size=(11,11) , strides=(4,4) , padding='same' , activation=tf.nn.relu,\n",
    "                     kernel_initializer = tf.initializers.truncated_normal(stddev=1e-2),\n",
    "                     bias_initializer = tf.initializers.constant(),\n",
    "                            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.00005),\n",
    "                            bias_regularizer=tf.contrib.layers.l2_regularizer(scale=0.00005))\n",
    "    lrn1 = tf.nn.local_response_normalization(conv1 , depth_radius=5 , alpha=0.0001 , beta=0.75 , bias=1.0)\n",
    "    maxpool1 = tf.layers.max_pooling2d(lrn1 , pool_size=(3,3) , strides=(2,2) , padding='valid')\n",
    "    \n",
    "    conv2 = tf.layers.conv2d(maxpool1 , filters=256 , kernel_size=(5,5) , strides=(1,1) , padding='same' , activation=tf.nn.relu,\n",
    "                     kernel_initializer = tf.initializers.truncated_normal(stddev=1e-2),\n",
    "                     bias_initializer = tf.initializers.constant(),\n",
    "                            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.00005),\n",
    "                            bias_regularizer=tf.contrib.layers.l2_regularizer(scale=0.00005))\n",
    "    lrn2 = tf.nn.local_response_normalization(conv2 , depth_radius=5 , alpha=0.0001 , beta=0.75 , bias=1.0)\n",
    "    maxpool2 = tf.layers.max_pooling2d(lrn2 , pool_size=(3,3) , strides=(2,2) , padding='valid')\n",
    "    \n",
    "    conv3 = tf.layers.conv2d(maxpool2 , filters=384 , kernel_size=(3,3) , strides=(1,1) , padding='same' , activation=tf.nn.relu,\n",
    "                     kernel_initializer = tf.initializers.truncated_normal(stddev=1e-2),\n",
    "                     bias_initializer = tf.initializers.constant(),\n",
    "                            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.00005),\n",
    "                            bias_regularizer=tf.contrib.layers.l2_regularizer(scale=0.00005))\n",
    "    \n",
    "    conv4 = tf.layers.conv2d(conv3 , filters=384 , kernel_size=(3,3) , strides=(1,1) , padding='same' , activation=tf.nn.relu,\n",
    "                     kernel_initializer = tf.initializers.truncated_normal(stddev=1e-2),\n",
    "                     bias_initializer = tf.initializers.constant(),\n",
    "                            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.00005),\n",
    "                            bias_regularizer=tf.contrib.layers.l2_regularizer(scale=0.00005))\n",
    "    \n",
    "    conv5 = tf.layers.conv2d(conv4 , filters=256 , kernel_size=(3,3) , strides=(1,1) , padding='same' , activation=tf.nn.relu,\n",
    "                     kernel_initializer = tf.initializers.truncated_normal(stddev=1e-2),\n",
    "                     bias_initializer = tf.initializers.constant(),\n",
    "                            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.00005),\n",
    "                            bias_regularizer=tf.contrib.layers.l2_regularizer(scale=0.00005))\n",
    "    \n",
    "    maxpool5 = spatial_pyramid_pool(conv5 , pyramid_bins)\n",
    "    \n",
    "    fc6 = tf.layers.dense(maxpool5 , units=1024 , activation=tf.nn.relu,\n",
    "                          kernel_initializer=tf.initializers.random_normal(stddev=1e-2),\n",
    "                          bias_initializer=tf.initializers.constant(),\n",
    "                          kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.00005),\n",
    "                          bias_regularizer=tf.contrib.layers.l2_regularizer(scale=0.00005))\n",
    "    fc6 = tf.layers.dropout(fc6 , keep_prob)\n",
    "    \n",
    "    fc7 = tf.layers.dense(fc6 , units=1024 , activation=tf.nn.relu,\n",
    "                          kernel_initializer=tf.initializers.random_normal(stddev=1e-2),\n",
    "                          bias_initializer=tf.initializers.constant(),\n",
    "                          kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.00005),\n",
    "                          bias_regularizer=tf.contrib.layers.l2_regularizer(scale=0.00005))\n",
    "    fc7 = tf.layers.dropout(fc7 , keep_prob)\n",
    "    \n",
    "    fc8 = tf.layers.dense(fc7 , units=num_classes,\n",
    "                          kernel_initializer=tf.initializers.random_normal(stddev=1e-2),\n",
    "                          bias_initializer=tf.initializers.constant(),\n",
    "                          kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.00005),\n",
    "                          bias_regularizer=tf.contrib.layers.l2_regularizer(scale=0.00005))\n",
    "    \n",
    "    return fc8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#构建计算图\n",
    "x = tf.placeholder(dtype=tf.float32 , shape=[batch_size , 224,224,3])\n",
    "y_ = tf.placeholder(dtype=tf.float32 , shape=[batch_size , num_classes])\n",
    "\n",
    "#logits = model(x)\n",
    "\n",
    "logits = model_finetune(x)\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_ , logits=logits))\n",
    "\n",
    "global_step = tf.Variable(0 , trainable=False)\n",
    "learning_rate = tf.train.exponential_decay(0.1 , global_step , decay_steps=1000 , decay_rate=0.9 , staircase=True)\n",
    "\n",
    "#train_step = tf.train.AdagradOptimizer(learning_rate).minimize(loss)\n",
    "train_step = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "#验证\n",
    "#softmax_logits = tf.nn.softmax(logits)\n",
    "#correct_count = tf.equal(tf.argmax(softmax_logits , axis=1) , tf.argmax(y_ , 1))\n",
    "\n",
    "correct_count = tf.equal(tf.argmax(logits , axis=1) , tf.argmax(y_ , 1))\n",
    "\n",
    "accu = tf.reduce_mean(tf.cast(correct_count , tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loss:4.627480 accu:0.000000\n",
      "2 loss:6.562114 accu:0.078125\n",
      "3 loss:5.553918 accu:0.046875\n",
      "4 loss:5.020390 accu:0.148438\n",
      "5 loss:5.026891 accu:0.039062\n",
      "6 loss:4.769906 accu:0.093750\n",
      "7 loss:4.351510 accu:0.085938\n",
      "8 loss:4.173891 accu:0.125000\n",
      "9 loss:4.181258 accu:0.171875\n",
      "10 loss:4.125919 accu:0.132812\n",
      "2 loss:3.993456 accu:0.179688\n",
      "3 loss:3.896168 accu:0.187500\n",
      "4 loss:4.020677 accu:0.125000\n",
      "5 loss:3.696506 accu:0.125000\n",
      "6 loss:3.691297 accu:0.195312\n",
      "7 loss:3.560419 accu:0.265625\n",
      "8 loss:3.554865 accu:0.320312\n",
      "9 loss:3.402827 accu:0.296875\n",
      "10 loss:3.438694 accu:0.242188\n",
      "11 loss:3.341337 accu:0.281250\n",
      "3 loss:3.136966 accu:0.390625\n",
      "4 loss:3.224842 accu:0.375000\n",
      "5 loss:2.925241 accu:0.312500\n",
      "6 loss:2.844578 accu:0.312500\n",
      "7 loss:3.116645 accu:0.296875\n",
      "8 loss:2.655392 accu:0.406250\n",
      "9 loss:2.758189 accu:0.484375\n",
      "10 loss:2.810175 accu:0.382812\n",
      "11 loss:2.527923 accu:0.453125\n",
      "12 loss:2.572539 accu:0.445312\n",
      "4 loss:2.431806 accu:0.460938\n",
      "5 loss:2.255583 accu:0.507812\n",
      "6 loss:2.216383 accu:0.445312\n",
      "7 loss:2.382442 accu:0.476562\n",
      "8 loss:2.265526 accu:0.453125\n",
      "9 loss:2.219739 accu:0.476562\n",
      "10 loss:1.998419 accu:0.507812\n",
      "11 loss:1.981065 accu:0.515625\n",
      "12 loss:2.146993 accu:0.468750\n",
      "13 loss:1.889982 accu:0.531250\n",
      "5 loss:2.103350 accu:0.468750\n",
      "6 loss:1.830079 accu:0.578125\n",
      "7 loss:1.922440 accu:0.562500\n",
      "8 loss:1.686129 accu:0.625000\n",
      "9 loss:1.648751 accu:0.585938\n",
      "10 loss:2.000119 accu:0.515625\n",
      "11 loss:1.481669 accu:0.687500\n",
      "12 loss:1.555503 accu:0.640625\n",
      "13 loss:1.467824 accu:0.640625\n",
      "14 loss:1.482939 accu:0.625000\n",
      "6 loss:1.473863 accu:0.640625\n",
      "7 loss:1.321989 accu:0.703125\n",
      "8 loss:1.542343 accu:0.625000\n",
      "9 loss:1.542334 accu:0.617188\n",
      "10 loss:1.399207 accu:0.640625\n",
      "11 loss:1.350573 accu:0.687500\n",
      "12 loss:1.221580 accu:0.679688\n",
      "13 loss:1.417403 accu:0.648438\n",
      "14 loss:1.354796 accu:0.640625\n",
      "15 loss:1.218652 accu:0.687500\n",
      "7 loss:1.281238 accu:0.632812\n",
      "8 loss:1.431345 accu:0.609375\n",
      "9 loss:1.223324 accu:0.742188\n",
      "10 loss:0.959904 accu:0.796875\n",
      "11 loss:1.375245 accu:0.695312\n",
      "12 loss:1.051725 accu:0.734375\n",
      "13 loss:1.078836 accu:0.750000\n",
      "14 loss:1.027148 accu:0.742188\n",
      "15 loss:0.970369 accu:0.796875\n",
      "16 loss:1.044445 accu:0.750000\n",
      "8 loss:1.093151 accu:0.726562\n",
      "9 loss:1.100709 accu:0.726562\n",
      "10 loss:1.137358 accu:0.734375\n",
      "11 loss:0.895228 accu:0.757812\n",
      "12 loss:0.914748 accu:0.742188\n",
      "13 loss:0.958738 accu:0.781250\n",
      "14 loss:0.802795 accu:0.789062\n",
      "15 loss:0.946908 accu:0.757812\n",
      "16 loss:0.998001 accu:0.734375\n",
      "17 loss:0.760842 accu:0.812500\n",
      "9 loss:1.042771 accu:0.734375\n",
      "10 loss:0.848431 accu:0.796875\n",
      "11 loss:0.687251 accu:0.828125\n",
      "12 loss:0.705712 accu:0.828125\n",
      "13 loss:0.791774 accu:0.828125\n",
      "14 loss:0.891558 accu:0.789062\n",
      "15 loss:0.817895 accu:0.812500\n",
      "16 loss:0.655981 accu:0.835938\n",
      "17 loss:0.833236 accu:0.820312\n",
      "18 loss:0.696968 accu:0.812500\n",
      "10 loss:0.639591 accu:0.875000\n",
      "11 loss:0.715914 accu:0.843750\n",
      "12 loss:0.765902 accu:0.828125\n",
      "13 loss:0.641363 accu:0.835938\n",
      "14 loss:0.664199 accu:0.835938\n",
      "15 loss:0.610189 accu:0.843750\n",
      "16 loss:0.684992 accu:0.812500\n",
      "17 loss:0.611800 accu:0.851562\n",
      "18 loss:0.658282 accu:0.835938\n",
      "19 loss:0.526253 accu:0.890625\n"
     ]
    }
   ],
   "source": [
    "#能收敛了 accu上升\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for it in range(10): #epoch=80\n",
    "\n",
    "        cnt_tmp = 0\n",
    "\n",
    "        for i in range(10):\n",
    "            it=it+1\n",
    "            \n",
    "            xtrain , ytrain = next_batch(batch_size=batch_size)\n",
    "\n",
    "            _ , train_accu , cost = sess.run([train_step , accu , loss] , feed_dict={x : xtrain , y_ : ytrain , keep_prob:1.0})\n",
    "\n",
    "            print('%d loss:%f accu:%f' % (it , cost , train_accu))\n",
    "\n",
    "            if train_accu>0.95:\n",
    "                cnt_tmp = cnt_tmp+1\n",
    "\n",
    "            if cnt_tmp>10:\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
